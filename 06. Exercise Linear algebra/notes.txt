Генерализацията на дължина я наричаме норма и я означаваме с по две черти. Имаме различни видове норми.
Втората норма ни дава Питагоровата теорема.
Векторизацията се използва много при невронните мрежи. Когато запазваме състоянието на една невронна мрежа то също е матрица, всъщност са много на брой матрици. Всеки слой от невронната мрежа има матрица, която запазва състоянието му.

Всеки пиксел е число от 0 до 255 (2 на 8ма - 1). 1 пиксел заема един байт.

Когато имаме едно пространство в него можем да си изберем произволна отправна система или гледна точка, която гледна точка наричаме базис. Тя служи за начин да разберем кое накъде е, кое в коя посока е, така да се каже. Ние познаваме Декартовата координатна система, но знаем, че има много други и базисите са генерализация на координатните системи. Така, че в едно пространство ние можем да си изберем най-подходяща за нас гледна точка. Освен това знаем, че има много такива. Реално са безкрайно много такива. Един базис и друг базис са напълно еквивалентни. Какво ни помага да изберем единия пред другия, това че нещо е по-удобно.
И това беше едната част от линейната алгебра - различни гледни точки в пространството, различни базиси.
Другата гледна точка беше трансформации. Тогава имаме линейна функция, която взима едно пространство и го трансформира в друго. При базисите, когато избираме базис характеристиките на векторите остават същите, променяме само гледната точка. При трансформацията векторите се променят, а пространството остава същото.

Линейните пространства понякога, освен всичко което имаме за тях, им закачаме дефиниция за скаларно произведение. Скаларното произведение прави от векторното пространство евклидово пространство. Всеки път, когато сложим някакво ограничение върху това което правим, първо правим по-интересни заключения. Колкото повече изисквания имаме към нещо, толкова по-конкретно знаем какво е и толкова повече негови свойства можем да изведем. Едно векторно пространство е абстрактно, върши работа за много случеи, но понеже е много абстрактно ние не можем да кажем твърде много неща за него. Ако му добавим нещо ограничаваме обхвата, който можем да получим, поставяме си ограничение. Но това ограничение пък, в случая необходимостта да има дефинирано скаларно произведение на вектори, което го нямаше в оригиналното векторно пространство. Но необходимостта от скаларно произведение ни позволява да направим много интересни неща - едно от които е да дефинираме разстояние. Оказа се, че когато говорим за машийн лърнинг се опират до разстояние, защото разстоянието е мярка за подобие. Ако два обекта имат малко разстояние един спрямо друг, значи са много подобни. Ако имат голямо разстояние един спрямо друг значи са много различни. И това е основата на някои ънсупервайз методи.

Знаем, че матриците са таблици от числа. Могат и да бъдат комплексни числа. Оказва се, че ако си мислим за матрици, линейни трансформации и т.н. можем да опишем много видове обекти с тях. Нещата, които можем да описваме с линейната алгебра: ако по някакъв начин имаме обекти или явления, които са независими, от тях винаги можем да пишем линейни комбинации. Това означава, че ако по някакъв начин, независимо точно какъв, имаме много на брой обекти, които всеки от тях си прави собственото си нещо и не си влияе един на друг, значи имаме основа за линейно описание. Това може да са и събития. Едно нещо, което се случва не влияе на друго нещо, което се случва. Това означава, че две неща такива стават едновременно, можем да вземем действието на едното отделно, действието на другото отделно и ще знаем, че общото действие е комбинация от двете действия.

Домашно! Композицията от линейни функции линейна ли е или не? (въпрос крайно необходим в невронните мрежи)