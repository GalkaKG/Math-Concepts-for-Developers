Предговор.
Има много фактори, които ни пречат да видим една реална стойност. Може би някъде тази стойност съществува, може би не съществува. Не знаем, честно казано не се и интересуваме много. Работата е там, че нещо ни хвърля прах в очите и този прах в очите ние трябва по някакъв начин да го премахнем. Да опитаме да прозрем отвъд проблемине, които са ни били вменени. Това става по много различни начини. Най-важният от които се нарича статистика. Нещо, което е много подобно като термини и като идеи са вероятностите. Вече имаме представа какво означава вероятност за някакво събитие.
Като знаем, че има 2 погледа върху тази вероятност. Едното от тях е правим n на брой експеримента, от които m се оказват успешни, такива които ни интересуват, такива които съдържат нашето събитие, каквото и да е то. Броя на интересните върху броя на общите наричаме измерена вероятност, експериментална вероятност. Математиката идва, когато броят на експериментите клони към безкрайност и тогава се получава нещо много интересно и което трябва някакси да имаме предвид. Ако броят на експериментите клони към безкрайност, то и броят на успешните клони към безкрайност. Но успешните са по-малко или равно на общия брой.
Колкото повече експерименти с функции правим, толкова повече експериментите се стабилизират, което е много важно когато работим с вероятности, данни и статистика. Колкото повече експерименти правим, толкова по-добре. Това правило си има име и името мъ е Закон за големите числа - "Law of large numbers". Всяка една случайна величина, когато я измерваме повече и повече пъти, някои нейни характеристики, например като средно аритметично, се стабилизират. Колкото повече неща наблюдаваме, толкова повече това, което наблюдаваме става по-близко до реалността.

Статистика е науката за количествено и качествено определяне на характеристики на случайни променливи. Казано малко по-просто правим много на брой наблюдения, виждаме какво се е случило и започваме да се питаме до колко това, което се е случило ни изненадва. Защото да ни изненада нещо или пък да не ни изненада много ние трябва да имаме някакво очакване за него. Такова очакване мога да имам - има термин математическо очакване. И в зависимост какво очакване имам за него, което очакване по-късно ще нарека хипотеза, резултатите, които наблюдавам могат малко или много да ме изненадат. малко или много да са различни от това очакване. И въпросът, който трябва да си зададем ще бъде: Окей, какъв е проблемът, моето очакване ли е грешно, данните ли са грешни или може би са толкова разнообразни, че не мога да преценя. И тук идва много важният въпрос, който в инженерството никъде го няма(почти никъде) и в дейта сайънс навсякъде го има - въпросът "Тази задача изобщо решима ли е, това което искам да постигна може ли изобщо да бъде постигнато? Тези данни достатъчни ли са? Колко данни трябва да имам?". И когато говорим за статистика като наука има и английски термин statistics, кеото може да бъде неброимо съществително статистика като наука, но имаме и one statistic, many statistics. One statistic е едно от числата, с които описваме някакво случайно събитие, да речем минималната стойност.
Статистиката не работи за едно наблюдение.
Когато говорим за статистиката като тема трябва да разделим 2 неща: 
- Едното е: ето ти тези данни, обясни какво е станало в тях, разполагаш с всички данни, които ме интересуват, разбери какво е станало в тях. Този вид статистика се нарича описателна или Descriptive.
- Другата, в която имаме ето тези данни, кажи какво ще стане в бъдеще е Inferential statistic, на български Статистика за предсказване.
В единия случай се питаме какво е било, в другия случай на база на това което е било искаме да разберем какво ще бъде по-нататък. Второто е по-интересно, но първото е много необходимо и без него второто не може да работи.
Колкото повече информация имам в началото толкова по-добре. И законът за големите числа не гарантира нищо какво правим с малко информация, с малко данни, с малко n, малко на брой записи. Ами отоговорът е каквото стане. Понякога малко са достатъчно. Най-доброто, което можем да направим е да предположим, че поведението на тези малко записи прилича на поведението на многото записи.
Малкото записи, записи означава редове в таблица от една страна и наблюдения от друга страна, тези малко данни се наричат извадка или Sample. Точно от там идва идеята за семплиране от едно разпределение. Имаме някакво разпределение, което означава че имаме променлива, случайно която има това разпределние и изкараме едно число от него. всяко едно число има различна вероятност. Нищо не ни пречи да изкараме просто по случайност число, което е било много малко вероятно. Еми окей, така се е случило - какво да направим. И съответно колкото по-голяма извадка правим от цялата съвкупност, толкова повече свойства на извадката наподобяват тези на съвкупността. Това беше законът за големите числа. И работата е там, че законът за големите числа не работи за малки и това можем само да го предположим. Т.е. можем само да предположим, че данните които имаме наподобяват много реалните. Това може да е грешно предположение. Абсолюно. Това е най-голямата и опасна грешка в статистиката и тя се нарича статистика баяс.
Ако някъде имаме баяс значи не сме отчели нещо. Тази грешка на практика е непоправима. Трябва да разберем, че бъркаме, за да може да я оправим. Много централен проблем. Ама айде да го преглътнем, защото без него нямаме никакво моделиране. Ако не допуснем възмжността да направим някаква такава грешка, да направим някакъв баяс, няма да можем да кажем нищо за никой процес. така че преглъщаме тази възможност да сме объркали, терминът който използваме  математиката е нашата извадка сампъл, да не бъде представителна, свойствата й да не наподобяват много тези на съвкупността. Една голяма част от работата ни ще бъде да търсим такива баясис, такива наклонности, грешки и т.н. и да ги отстраняваме, ако това е възможно. това е както науката, така и изкуството в дейта сайънс. Защото формула за това няма. Винаги има някакъв външен фактор, който може да каже дали има грешка или не.
Никоя система, която е консистентна не може да каже сама за себе си, че е консистентна. Това се нарича теорема на Гьодел.
Никоя система не може да каже сама, че е сбъркала. Можем да разберем по някакъв външен начин.

Какви методи на семлиране има?
- Най-основният от тях е случайно семплиране. Всеки елемент има равна вероятност да бъде избран.
i.i.d. (independent and dynamically distributed) - съкращение за случайни променливи.

Категорийна променлива е такава, която има ограничен брой стойности.

Корелация
Това, че между две променливи има корелация не означава, че между тях има причинноследствена връзка. Ако a е корелирано с b това не означава, че задължително a е причина за b. Може връзката да е друга, може b да е причина за a, може някакъв външен фактор да причинява и едното и другото, може външният фактор да причинява b, което да причинява друг външен фактор, което да причинява a. Има много много варианти. Корелацията между две променливи е полезна. Тя ни дава информация, че когато едното нараства, да речем другото нараства заедно с него или другото намалява. Това е полезна информация, която може да използваме, може да експлоатираме.
Когато обаче правим анализ на причинно-следствените връзки, кое за кое е причина, това не е достатъчно.

Парадокс на Симсън.
